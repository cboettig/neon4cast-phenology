---
title: "EFI NEON Phenology forecasting challenge"
author: "EFI NEON Phenology working group"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(scales)
library(colorBlindness)
library(dplyr)
```

Intro, Methods, Discussion, etc will be developed in Google Docs

## Results

```{r,echo=FALSE}
## grab and load forecasts
filename = "phenology_combined_forecasts.csv"
if(!file.exists(filename)){
  object <- aws.s3::get_bucket("forecasts",
                               prefix = paste0("not_in_standard/",filename),
                               region = "data",
                               base_url = "ecoforecast.org")
  aws.s3::save_object(object[[1]], 
                        bucket = "forecasts", 
                        file = filename,
                        region = "data",
                        base_url = "ecoforecast.org")
}
submittedForecasts <- readr::read_csv(filename,col_names=TRUE)
```


Prep:
**Inventory grid of models by dates, count sites submitted (supplementary figure)**

**ID transition dates that actually occurred at each site**
Could organize everything into a dataframe thatâ€™s model, startdate, <existing cols>
```{r}
transDateFile = "allPhenologyTransitionData.csv"
if(!file.exists(transDateFile)){
  source("calculatePhenoCamTransitionDates.R")
}
allTransitions <- readr::read_csv(transDateFile,col_names = TRUE)
s=1
tms <- unique(submittedForecasts$team)
site_names <- c("HARV", "BART", "SCBI", "STEI", "UKFS", "GRSM", "DELA", "CLBJ")
```

## Figure 1: Dates of forecast and submission figure 
```{r, echo=FALSE}
## Based on Kathryn's code in ESA2021_PresentationFigures.R
tranDates <- as.Date(allTransitions[,2],origin=as.Date("2020-12-31"))
tranDates <- c(tranDates,as.Date(allTransitions[,6],origin=as.Date("2020-12-31")))
challengeDays <- seq(as.Date("2021-02-01"),as.Date("2021-06-30"),"day")

##jpeg(file="DatesOfForecastAndSubmissionFigure.jpg",width = 1000, height = 480, units = "px")#,height=3,width=10,units="inch",res=700)

par(mfrow=c(1,2),mai=c(1,2,0.3,0.1))

#for(s in 1:length(site_names)){
submittedForecastsYes <- data.frame(matrix(ncol=length(tms),nrow=length(challengeDays)))
for(tm in 1:length(tms)){
  tmDat <- submittedForecasts[submittedForecasts$team==tms[tm],] #Subset by team
  tmSitDat <- tmDat[tmDat$siteID==site_names[s],] #Subset by site
  uniqueTimes <- unique(as.Date(tmSitDat$time))
  for(d in 1:length(challengeDays)){
    submittedForecastsYes[d,tm] <- challengeDays[d] %in% uniqueTimes
  }
}
colnames(submittedForecastsYes) <- tms

#Plot first team and then add on subsequent teams onto the graph
tm=1

plot(challengeDays[submittedForecastsYes[,tm]],rep(tm,length(challengeDays))[submittedForecastsYes[,tm]],pch=20,
     xlab="Time",ylab="",ylim=c(0,length(tms)),bty="n",yaxt="n",main="Forecasted Days")
axis(side = 2,at=seq(1,length(tms)),labels=tms,pos=as.Date("2021-01-20"),las=1)

polygon(x=c(min(tranDates),min(tranDates),max(tranDates),max(tranDates)),y=c(-0.9,length(tms)+1,length(tms)+1,-0.9),col="chartreuse3",border=NA)
for(tm in 1:length(tms)){
  points(challengeDays[submittedForecastsYes[,tm]],rep(tm,length(challengeDays))[submittedForecastsYes[,tm]],pch=20)
}

#Plot dates of submissions 
submittedForecastsYes <- data.frame(matrix(ncol=length(tms),nrow=length(challengeDays)))

for(tm in 1:length(tms)){
  tmDat <- submittedForecasts[submittedForecasts$team==tms[tm],] #Subset by team
  tmSitDat <- tmDat[tmDat$siteID==site_names[s],] #Subset by site
  uniqueTimes <- unique(as.Date(tmSitDat$forecast_start_time))
  for(d in 1:length(challengeDays)){
    submittedForecastsYes[d,tm] <- challengeDays[d] %in% uniqueTimes
  }
}
colnames(submittedForecastsYes) <- tms

tm=1
par(mai=c(1,0.1,0.3,2))
plot(challengeDays[submittedForecastsYes[,tm]],rep(tm,length(challengeDays))[submittedForecastsYes[,tm]],pch=20,
     xlab="Time",ylab="",ylim=c(0,length(tms)),bty="n",yaxt="n",main="Submission Days",xlim=range(challengeDays))

polygon(x=c(min(tranDates),min(tranDates),max(tranDates),max(tranDates)),y=c(-0.9,length(tms)+1,length(tms)+1,-0.9),col="chartreuse3",border=NA)
for(tm in 1:length(tms)){
  points(challengeDays[submittedForecastsYes[,tm]],rep(tm,length(challengeDays))[submittedForecastsYes[,tm]],pch=20)
}
#}
##dev.off()
```




# Analyses

**Example time series: individual sites, specific forecast dates, multiple models**
Goal: visualization

```{r, echo=FALSE}
## Time series figures
s=1
targetDay = allTransitions$day15[s]
## find the day closest to the target date that had the most forecasts submitted
window = 5
targetRows = which(lubridate::yday(challengeDays) %in% ((-window:window)+targetDay))
submitted = apply(submittedForecastsYes[targetRows,],1,sum)
startDate = challengeDays[as.numeric(names(which.max(submitted)))]
## grab and organize forecasts
ts = subset(submittedForecasts, forecast_start_time == startDate & siteID == site_names[s])
ts.teams = unique(ts$team)
ylim = c(min(ts$gcc_90[ts$statistic == "lower95"]),max(ts$gcc_90[ts$statistic == "upper95"]))
xlim = c(startDate,startDate+35)
extract.ts = tapply(1:nrow(ts),ts$team,function(x){
  dat = ts[x,]
  y = dat$gcc_90[dat$statistic=="mean"]
  yhi = dat$gcc_90[dat$statistic=="upper95"]
  ylo = dat$gcc_90[dat$statistic=="lower95"]
  tm = dat$time[dat$statistic=="mean"]
  return(data.frame(tm,ylo,y,yhi))
  })
plot(extract.ts[[1]]$tm,extract.ts[[1]]$y,ylim=ylim,xlim=xlim,
     xlab="date",ylab="gcc_90",type='n')
for(i in seq_along(ts.teams)){
  lines(extract.ts[[i]]$tm,extract.ts[[i]]$y,col=i)
}
  
```

**Skill vs lead time for different parts of the season [Kathryn, David, Arun]**
Goal: using key date thresholds as examples, determine predictability
  Does this vary by threshold? (e.g. is it easier to predict 50% expansion than 15%)
  Does this vary by type of model, complexity, driver, and type uncertainties considered?
  What does this tell us about overall predictability and what forecasting approaches are most promising
Analysis
  Determine when specific thresholds (15%, 50%, 85%) were reached by site. Method? (logistic? Moving average?)
  For 0 to 35 days ahead of each threshold, extract what each model predicted on that date
  Calculate: CRPS, MAE, bias, [0.025, 0.5, 0.975] quantiles (for visualizing)
  Visualize: Individual sites & thresholds, multiple models
    Ways of summarizing: long lead skill?, rate/degree of convergence? 
  Use linear models to assess what factors affected predictability
    like which factors? - model complexity, training data, sources of uncertainty, forecast horizon, site?, adjacent training data
```{r, echo=FALSE}
## Lead Time Figures
```


## Figure 2: Changes in forecasted values on transition dates
```{r, echo=FALSE}
## Based on Kathryn's code in ESA2021_PresentationFigures.R

s=1

sitDat <- submittedForecasts[submittedForecasts$siteID==site_names[s],] #Subset by site
cls <- c("#004949","#000000",paletteMartin[3:15])

##pdf(file="ForecastedValuesOnTransitionDates_presentationFigures.pdf",height=5,width=12)
par(mfrow=c(1,4),mai=c(0.5,0.5,1,0.1))

plotForecastedValuesOverTime <- function(highlightTms=NA){
  finalTms <- character()
  for(t in c(2,5,8)){ #Loops over the transition dates
    cl <- 1
    tranDate <- as.Date(allTransitions[s,t],origin=as.Date("2020-12-31"))
    vl <- allTransitions[s,(t+1)]
    sdVal <- allTransitions[s,(t+2)]
    vl <- rescale(vl,to=c(0,1),from=c(allTransitions$minimum[s],allTransitions$maximum[s])) ##Rescales gcc values between 0 and 1
    plot(x=numeric(),y=numeric(),type="l",xlim=c(-35,0),ylim=c(0,1),ylab="",xlab="Days Before Transition Date",main=paste(site_names[s],tranDate),bty="n")
    
    abline(h=vl,col="red",lwd=5,lty=2)
    for(tm in 1:length(tms)){
      tmSitDat <- sitDat[sitDat$team==tms[tm],] #Subset by team
      predDat <- tmSitDat[as.Date(tmSitDat$time)==tranDate,] #Subset of the forecasts that forecasted the transition date 
      
      #Organizing data to put upper95, lower95, and mean on the same row 
      organizedDat <- predDat[predDat$statistic=="mean",c("time","forecast_start_time","gcc_90")]
      organizedDat <- organizedDat %>% rename("mean"="gcc_90")
      lowDat <- predDat[predDat$statistic=="lower95",c("time","forecast_start_time","gcc_90")]
      lowDat <- lowDat %>% rename("lower95"="gcc_90")
      highDat <- predDat[predDat$statistic=="upper95",c("time","forecast_start_time","gcc_90")]
      highDat <- highDat %>% rename("upper95"="gcc_90")
      organizedDat <- merge(organizedDat,lowDat,by=c("time","forecast_start_time"))
      organizedDat <- merge(organizedDat,highDat,by=c("time","forecast_start_time"))
      if(nrow(organizedDat)>0){
        if(t==8){
          finalTms <- c(finalTms,as.character(tms[tm]))
        }
        sitTmMax <- max(tmSitDat$gcc_90[tmSitDat$statistic=="mean"],na.rm=TRUE)
        rescaledDat <- rescale(organizedDat$mean,to=c(0,1),from=c(allTransitions$minimum[s],sitTmMax)) #Rescales forecasted values between 0 and 1
        for(j in 1:length(rescaledDat)){ #Some values get scaled below 0 
          rescaledDat[j] <- max(rescaledDat[j],0)
        }
        if(is.na(highlightTms) || tm%in%highlightTms){ #No transparency if you do not want to highlight teams 
                                                       #or if the team is within the highlighted teams
          tF <- 1
          lwdVl <- 3
        }else{
          tF <- 0.2
          lwdVl <- 1
        }
        lines(as.Date(organizedDat$forecast_start_time)-tranDate,rescaledDat,col=scales::alpha(cls[cl],tF),lwd=lwdVl)
        cl <- cl + 1
      }
    }
  } 
  plot(x=numeric(),y=numeric(),type="l",xlim=c(-35,0),ylim=c(0,1),ylab="",xlab="",main="Legend",bty="n")
  legend("topleft",c("True Value",as.character(finalTms)),col=c("red",cls[1:length(finalTms)]),lty=c(2,rep(1,length(finalTms))),lwd=c(2,rep(3,length(finalTms))),bty = "n")
}
plotForecastedValuesOverTime()
plotForecastedValuesOverTime(highlightTms = 12)
plotForecastedValuesOverTime(highlightTms = 1)
plotForecastedValuesOverTime(highlightTms = 13)
plotForecastedValuesOverTime(highlightTms = c(2,11))
##dev.off()

```

```{r, echo=FALSE}
## Lead Time Stats
```


**CRPS through time: individual sites, multiple models, specific lead times**
Goal: Generalize what we learned from previous analysis continuously
Use the results from previous analysis to propose some specific lead times that are interesting to look at (e.g. 1, 2, 3 week)
Models may have consistent biases (high/low, early/late); might catch general shape but be over/underpredicting gcc
```{r,echo=FALSE}
## CRPS figures
```


**Additional analyses???**
Reminder: there will be future rounds & future papers (more sites, more years)
Table with aggregate scores - but have to be careful to count for when there are just forecasts for the easy times or for forecasts submitted every 5 days vs those submitted every day


